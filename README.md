# Crawlers for automatic article collection from online papers for the corpus of Tajik language

everyday.py downloads papers from the previous day
archive.py downloads archive of articles all the way untill the day of script execution

python packages required: pandas

authors: M. Tsfasman & D. Ignatenko
___________________________________

Краулер был написан в 2020-м, после этого я его переписывала и обновляла. Я использовалась только код для выкачивания газет архивом (от даты, когда газеты выкачивались в последний раз, до нынешнего дня).
Сейчас это 30.01.2023. Это важно, потому что в некоторых объектах краулеров дата, с которой нужно начинать скачивание, прописывается хардкодно -- ее нужно менять.
Этот пункт можно переписать, но в краулере и так огромное количество переменных, что для наглядности я использовала так.

Сайт каждой газеты специфичен и время от времени меняется, поэтому я рекомендую запускать по одному краулеру за раз и отслеживать возможные ошибки.

Здесь краулеры для газет:
Faraj, Ovozi, Ovozi Samarqand, Asia-Plus TJ, Oila, Khovar, Sputnik Tajikistan, Ozodi.
Для краулера для Ozodi необходимо настроить скачивание через другой IP-адрес, из РФ газета заблокирована.

Примечание:
1. Для краулера Ovozi Samarqand нужно вручную указывать диапазон id газет, которые нужно скачать. Его нужно определить, проверив на сайте номер последний вышедшей газеты. К нему добавляется номер последней скачанной газеты (январь 2023).
2. Структура газеты Ovozi хитрая. Насколько я помню, на последующих страницах могут отображаться новости за даты, которые не идут по порядку (т.е. скачивается диапазон новостей августа 2022, а по какой-то причине сайт отображает новость июня 2023 на этой странице. Конечно, этой новости в это части не должно быть.). Есть подобная проблема, код ее решает.
3. В газете Asia-Plus TJ сайт переставал отображать новости после 13-й страницы. Как сейчас, не знаю. Но я писала костыль для этой проблемы, который по очереди выкачивал последующие батчи новостей.
4. Краулер для газеты Ozodi работал так: сначала собирались ссылки на страницы новостей по категориям, и только потом ссылки обрабатывались. Малейшая ошибка - скачивание нужно начинать заново, и нужно заново скачивать ссылки. В коде есть костыль, который предварительно выкачивает все нужные ссылки и сохраняет их в отдельный .txt-файл. Уже после этого краулер итерируется по этому файлу, и ему не нужно предварительно выкачивать ссылки.

Пожалуй, это все костыли.


При написании новых краулеров газет я рекомендую:
1. Воспользоваться структурой, которую авторки задали изначально: написать объект краулера, который наслудется от класса Crawler. После этого нужно заменить методы выкачивания газет. Я рекомендую наследоваться от объекта, потому что в нем аккуратно прописано хранение данных и обработка мета-данных.
2. Писать новые методы лучше самым простым путем: скачивать веб-страницу и обрабатывать через BeautifulSoup. Изначально авторки делали иначе, но на мой взгляд так проще и чище, и я переписала код.
3. Также есть clean_everything.py , который применяется, чтобы дополнительно почистить содержание газет.
4. После скачивания газет нужно обработать файл с метаданными скриптом metaforcorpus.py. Он переведет формат, в котором краулер сохраняет метаданные, в формат в котором метаданные можно включать в корпус, так как они не совпадают.

5. Также из неочевидного: для правильной обработки дат нужно менять локаль на таджикскую. Наименование/сокращения месяцев не везде совпадают, от этого падают ошибки.

